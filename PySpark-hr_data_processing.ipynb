{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0e7b3a",
   "metadata": {},
   "source": [
    "# PySpark Project - Analyzing HR Data\n",
    "\n",
    "### Objective:\n",
    "\n",
    "***The aim of this project is to analyze HR data regarding employee attrition & performance using PySpark (a Python API for Spark) to query and process data***\n",
    "\n",
    "* **Loading data from CSV into a Spark DataFrame**\n",
    "* **Analyze data in a Spark DataFrame using SQL**\n",
    "* **Saving transformed data back to disk**\n",
    "\n",
    "_The Dataset used in this project is available from:_\n",
    "\n",
    "<u>_Note:_</u> The project is commented throughout to aid readability and support readers with different background knowledge to understand the code presented in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a0d5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "# ! pip install pyspark # Uncomment this line if pyspark module is not installed\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0208ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(spark, path, **options):\n",
    "    \"\"\"\n",
    "    Function to load a CSV file defined in the path with configuration options\n",
    "    @params:\n",
    "        - 'spark': the SparkSession object\n",
    "        - 'path: the path to the dataset\n",
    "        - 'options': configuration options for how the dataset should be loaded\n",
    "    @return: DataFrame object\n",
    "    \"\"\"\n",
    "    \n",
    "    data = spark.read.options(**options).csv(path)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6bf516c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_values(df):\n",
    "    \"\"\"\n",
    "    Function to count the number of unique values for each column in the Spark DataFrame\n",
    "    @params:\n",
    "        - 'df': the DataFrame to use\n",
    "    @return: A Python Dictionary with the following format: {column name: number of unique values}\n",
    "    \"\"\"\n",
    "    \n",
    "    col_dist_values = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Use the 'collect_set' function to get all the unique values for each column\n",
    "        num_unique_values = df.select(collect_set(col))\n",
    "        # print(num_unique_values)  # DataFrame[collect_set(BusinessTravel): array<string>]\n",
    "        # print(num_unique_values.first())  # Row(collect_set(BusinessTravel)=['Travel_Frequently', 'Non-Travel', 'Travel_Rarely'])\n",
    "        \n",
    "        # Get the length of the array stored in the Row object and add it to the dictionary with the name of the\n",
    "        # column as the key\n",
    "        col_dist_values[col] = len(num_unique_values.first()[0])\n",
    "    \n",
    "    return col_dist_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16c588a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df, columns, show=False):\n",
    "    \"\"\"\n",
    "    Function to reduce dimension by selecting a subset of columns in the DataFrame\n",
    "    @params:\n",
    "        - 'df': the DataFrame to use\n",
    "        - 'columns': the columns to be selected\n",
    "        - 'show': whether or not to show the first 5 rows of the new DataFrame (default: False)\n",
    "    @return: a new DataFrame object\n",
    "    \"\"\"\n",
    "    \n",
    "    subset_data = df.select(columns)\n",
    "    \n",
    "    if show:\n",
    "        subset_data.show(5, truncate=False)\n",
    "    \n",
    "    return subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d05790dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(spark, query, show=False):\n",
    "    \"\"\"\n",
    "    Function to run the SQL query passed in as an argument using the current SparkSession\n",
    "    @params:\n",
    "        - 'spark': the current SparkSession object\n",
    "        - 'query': the SQL query to be executed\n",
    "        - 'show': whether or not to show the first 5 rows of the result of the query (default: False)\n",
    "    @return: the DataFRame returned from the query\n",
    "    \"\"\"\n",
    "    \n",
    "    query_result = spark.sql(query)\n",
    "    \n",
    "    if show:\n",
    "        query_result.show(5, truncate=False)\n",
    "    \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aaef19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, path, mode='overwrite', _format='pq'):\n",
    "    \"\"\"\n",
    "    Function to write a DataFrame to disk\n",
    "    @params:\n",
    "        - 'df': the DataFrame to be saved\n",
    "        - 'path': the path where the data should be saved to\n",
    "        - 'mode': defines what happens if the data already exists (default: overwrite)\n",
    "        - '_format': the format the data should be saved in (default: parquet)\n",
    "    \"\"\"\n",
    "    \n",
    "    if _format == 'csv':\n",
    "        df.write.csv(path, mode)\n",
    "    else:\n",
    "        df.write.parquet(path, mode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "085aa7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    The main function to execute the code\n",
    "    \"\"\"\n",
    "    \n",
    "    APP_NAME = 'PySpark Project - HR Data Processing'  # Define the name of the project\n",
    "    DATASET_PATH = './Assets/Datasets/ibm_hr_analytics_employee_attrition_&_performance.csv' # NOTE: The dataset \n",
    "    # path might differ depending on where the dataset is saved\n",
    "    WRITE_PATH = './Assets/Datasets/hr_data_processing/'  # Define the path where transformed data should be saved\n",
    "    # to\n",
    "\n",
    "    # 1. - LOADING DATA FROM CSV TO A SPARK DATAFRAME\n",
    "    \n",
    "    # Create new SparkSession and assign it to a variable 'spark' (this variable then can be used to reference the\n",
    "    # SparkSession object later on in the code)\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "    \n",
    "    # Define configuration for loading in the dataset\n",
    "    df_config = {\n",
    "        'header': True,  # Indicate that the dataset already contains a header\n",
    "        'delimiter': ',',  # Indicates that each column is separated by a comma (',') in the CSV\n",
    "        'inferSchema': True  # Sample a subset of data to determine column types\n",
    "    }\n",
    "    \n",
    "    # Call the 'load_dataset' function to load in the data\n",
    "    hr_data = load_dataset(spark, DATASET_PATH, **df_config)\n",
    "    \n",
    "    hr_data.printSchema() # Check the DataFrame structure, columns, and data types\n",
    "    \n",
    "    # 2. ANALYZING DATA IN A SPARK DATAFRAME USING SQL\n",
    "    \n",
    "    # Check the number of distinct values for each column\n",
    "    num_dist_values = get_distinct_values(hr_data)\n",
    "    \n",
    "    # Select all the columns that have the same value for evey observation in the DataFrame\n",
    "    cols_to_drop = [col for col, count in num_dist_values.items() if count == 1]\n",
    "    \n",
    "    # print(f'Columns with variance of 0: {cols_to_drop}')\n",
    "    \n",
    "    # No extra information can be derived from columns with variance of 0 since it's the same for all observations,\n",
    "    # hence these columns will be dropped - the 'EmployeeNumber' column will also be dropped since it only contains\n",
    "    # internal administrative information\n",
    "    hr_data = hr_data.drop('EmployeeNumber', *cols_to_drop)\n",
    "    \n",
    "    # print(hr_data.columns)\n",
    "    \n",
    "    # At this point, the DataFrame is ready to be analyzed with SQL, however if one wants to further reduce the\n",
    "    # number of columns to work on, it can be done using the 'select_columns' function\n",
    "   \n",
    "    # columns_to_use = []  # List the name of the columns to use for analysis\n",
    "    # hr_data = select_columns(columns_to_use)\n",
    "    \n",
    "    hr_data.createOrReplaceTempView('hr_data')  # Create a temporary view of the DataFrame in memory to allow to \n",
    "    # allow data manipulation on the data using SQL\n",
    "    \n",
    "    # Define queries\n",
    "    query_1 = \"\"\"\n",
    "        SELECT Department, AVG(HourlyRate) FROM hr_data\n",
    "        GROUP BY 1\n",
    "        ORDER BY 2 DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    query_2 = \"\"\"\n",
    "        SELECT Department, JobRole, COUNT(*) AS Total FROM hr_data\n",
    "        WHERE JobSatisfaction == 4\n",
    "        GROUP BY 1, 2\n",
    "        ORDER BY 3 DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    # Collect queries in a dictionary - (query name: query) format\n",
    "    queries = {    \n",
    "        'department_hourly': query_1,\n",
    "        'department_satisfaction': query_2\n",
    "    }\n",
    "    \n",
    "    # Loop through the query dictionary, execute the query and save it to disk \n",
    "    for df, query in queries.items():\n",
    "        query_result = execute_query(spark, query)\n",
    "        query_result.show(truncate=False)\n",
    "        \n",
    "        # 3. SAVING QUERY RESULTS TO DISK\n",
    "        \n",
    "        save_dataset(query_result, os.path.join(WRITE_PATH, df))\n",
    "    \n",
    "    # Stop the SparkSession\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dd33390a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      " |-- BusinessTravel: string (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EducationField: string (nullable = true)\n",
      " |-- EmployeeCount: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- Over18: string (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StandardHours: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      "\n",
      "+----------------------+-----------------+\n",
      "|Department            |avg(HourlyRate)  |\n",
      "+----------------------+-----------------+\n",
      "|Research & Development|66.16753381893861|\n",
      "|Sales                 |65.5201793721973 |\n",
      "|Human Resources       |64.3015873015873 |\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------------+-----+\n",
      "|Department            |JobRole                  |Total|\n",
      "+----------------------+-------------------------+-----+\n",
      "|Sales                 |Sales Executive          |112  |\n",
      "|Research & Development|Research Scientist       |95   |\n",
      "|Research & Development|Laboratory Technician    |80   |\n",
      "|Research & Development|Healthcare Representative|43   |\n",
      "|Research & Development|Manufacturing Director   |38   |\n",
      "|Sales                 |Sales Representative     |23   |\n",
      "|Research & Development|Research Director        |22   |\n",
      "|Research & Development|Manager                  |17   |\n",
      "|Human Resources       |Human Resources          |13   |\n",
      "|Sales                 |Manager                  |12   |\n",
      "|Human Resources       |Manager                  |4    |\n",
      "+----------------------+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
